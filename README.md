# 🕵️ AI 힌트 추리 게임 - 정체를 밝혀라!

AI가 한 인물을 정하고, 사용자는 제한된 질문을 통해 그 정체를 맞추는 Streamlit 기반 인터랙티브 게임입니다.

---

## ✨ 주요 기능

- 사용자는 최대 **10개의 질문**을 통해 인물을 유추
- AI는 질문에 대해 **힌트를 제공**, 단 정답을 직접 말하지 않음
- 사용자가 **정확히 이름을 말할 경우**에만 “정답이야!” 응답
- 10개 질문 이후에도 못 맞히면, AI가 **정답 공개**
- 질문 수 카운트, 분야 선택 가능

---

## 🧱 기술 구성

- **프론트엔드**: Streamlit
- **백엔드 모델**: Ollama + EEVE-Korean-10.8B
- **배포 환경**: RunPod PyTorch + Jupyter 환경

---

## 🛠️ 실행 순서

### 1. RunPod에서 Pod 생성
- 템플릿: `RunPod Pytorch 2.1.1 + Jupyter`
- 설정 완료 후 Web Terminal 접속

### 2. Ollama 설치 및 서버 실행
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

### 3. 모델 다운로드
```bash
ollama pull EEVE-Korean-10.8B
```

### 4. 게임 실행
```bash
streamlit run LLM_game.py --server.address 0.0.0.0 --server.port 8000
```

### 5. 접속 방법
- RunPod가 부여한 외부 IP + 포트 (예: `69.30.xx.xx:22098`)로 접속

---

## 🧠 LLM 모델 적용 방식

이 게임은 **EEVE-Korean-10.8B**라는 **한국어 특화 LLM**을 활용하여, AI가 인물처럼 답변하는 자연스러운 대화를 구현합니다.

---

### ✅ 적용 방식

- **Ollama 서버**에 LLM을 실행하고, Streamlit 앱에서는 **프롬프트 기반 채팅 API**처럼 활용
- 게임 시작 시, 선택된 인물의 설명 (`descriptions.json` 기반)을 **프롬프트에 포함시켜** AI가 해당 인물인 것처럼 행동함
- 사용자의 질문이 들어오면, **모델은 해당 인물의 정보에 기반하여 힌트를 간접적으로 생성**
- **정답 여부 판별은 LLM이 아닌 코드**에서 직접 처리 (`"정답이야!"` / `"아니야!"`)

---

### 🧠 모델의 역할

- 사용자의 질문에 대해 **자연스럽고 명확한 힌트를 생성**  
  > 예: “무대에서 에너지가 넘치는 퍼포먼스를 보여주는 사람입니다.”

- 반드시 `descriptions.json`에 있는 **사실 기반 정보만 사용**  
- 인물의 **이름은 절대 언급하지 않지만**,  
  - **별명**
  - **소속 그룹**
  - **출연작** 등은 힌트로 활용 가능

---


---

## 🛡️ 주의사항 및 오류 방지

- `ollama serve`는 반드시 백그라운드에서 **실행 중이어야 함**
- 모델명 정확히 입력해야 함: `EEVE-Korean-10.8B`
- 모델 미설치 시 오류 발생 → `ollama pull`로 사전 다운로드 필요

---

## 📎 기타 정보

- 인물은 아이돌, 스포츠 선수, 캐릭터 등 100여 명으로 구성 (descriptions.json파일)
- 캐릭터 카테고리 **선택 가능**

---

## 🎉 결과 예시

- 질문 10번 내에 맞추면 🎊 축하 메시지와 함께 정답 공개
- 실패 시 ❌ 에러 메시지와 함께 정답 안내

---

